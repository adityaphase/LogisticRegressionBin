{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall start by importing the numpy and pandas libraries. The use of numpy is to manage and handle arrays in a better fashion and we shall be using the basic ability of pandas to import data ( excel style .csv files ).The dataset we shall be using is a simple 2 feature dataset ( the dataset can be found on my GitHub )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming file is located on desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(‘/Users/nuser/Desktop/ex2data1.csv’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall be using the iloc indexer for selecting the appropriate array from the dataset;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.iloc[:, [0, 1]].values\n",
    "y = dataset.iloc[:, [2]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need data for training and some more unique data for testing. We all are aware that using the training data for testing is a real bad idea since it effectively cheats the algorithm and hence tends to perform poorly in unique data scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall be using 75 % of our data for training and rest for testing our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split( x, y, test_size = 0.25, random_state = 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "——————^_^ I expect you to come here again ^_^ ———————"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train the model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver=‘lbfgs’, random_state = 0)\n",
    "lr.fit( xtrain, ytrain.ravel( ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ravel() method for taking transpose on ytrain vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = lr.predict( xtest )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find the accuracy of our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(“Accuracy:  ”, accuracy_score(ytest, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data we get a accuracy of 80% which isn’t really great but it can be hugely improved if we use feature scaling. Feature scaling can be done using the StandardScaler method. Feature scaling is basically converting the elements of our dataset to be in the same domain thereby preventing the model to be biased or inclined towards a single or a specific set of values from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sx = StandardScaler()\n",
    "xtrain = sx.fit_transform(xtrain)\n",
    "xtest = sx.transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing feature scaling before training the algorithm ( goto “ ^_^ I expect you to come here again ^_^ “)  improves our accuracy by a significant amount bringing it to 92 % which is not too shabby.( The full script can be found on my GitHub)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
